{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adedfa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Coding\\AIOprj2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8cdd824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ƒê·ªçc b·ªô d·ªØ li·ªáu\n",
    "DATASET_PATH = \"2cls_spam_text_cls.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "# T√°ch tin nh·∫Øn v√† nh√£n v√†o c√°c list\n",
    "messages = df[\"Message\"].values.tolist()\n",
    "labels = df[\"Category\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a656dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Load m√¥ h√¨nh embedding\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# H√†m ƒë·ªÉ tr√≠ch xu·∫•t embedding t·ª´ output c·ªßa model\n",
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    # Fix: Use ~attention_mask.bool() to mask padding tokens, not attention_mask.bool()\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0dff0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/175 [00:00<?, ?it/s]c:\\Coding\\AIOprj2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175/175 [04:31<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# 3.2. T·∫°o sentence embeddings\n",
    "def get_embeddings(texts, model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"T·∫°o embeddings cho m·ªôt danh s√°ch c√°c vƒÉn b·∫£n\"\"\"\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_texts_with_prefix = [f\"passage: {text}\" for text in batch_texts]\n",
    "\n",
    "        batch_dict = tokenizer(batch_texts_with_prefix, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_dict)\n",
    "            batch_embeddings = average_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
    "            embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Chu·∫©n b·ªã nh√£n\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "# T·∫°o embeddings cho t·∫•t c·∫£ tin nh·∫Øn\n",
    "X_embeddings = get_embeddings(messages, model, tokenizer, device)\n",
    "# T·∫°o metadata cho m·ªói t√†i li·ªáu\n",
    "metadata = [{\"index\": i, \"message\": message, \"label\": label, \"label_encoded\": y[i]}\n",
    "             for i, (message, label) in enumerate(zip(messages, labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a13550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3. T·∫°o FAISS index v√† chia d·ªØ li·ªáu\n",
    "TEST_SIZE = 0.1\n",
    "SEED = 42\n",
    "train_indices, test_indices = train_test_split(range(len(messages)), test_size=TEST_SIZE, stratify=y, random_state=SEED)\n",
    "\n",
    "# T√°ch embeddings v√† metadata theo ch·ªâ s·ªë ƒë√£ chia\n",
    "X_train_emb = X_embeddings[train_indices]\n",
    "X_test_emb = X_embeddings[test_indices]\n",
    "\n",
    "train_metadata = [metadata[i] for i in train_indices]\n",
    "test_metadata = [metadata[i] for i in test_indices]\n",
    "\n",
    "# T·∫°o FAISS index\n",
    "embedding_dim = X_train_emb.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "index.add(X_train_emb.astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7183a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tri·ªÉn khai ph√¢n lo·∫°i v·ªõi embedding similarity\n",
    "\n",
    "def evaluate_knn_accuracy(test_embeddings,test_metadata,index, train_metadata,k_values=[1,3,5]):\n",
    "    \"\"\"Evaluate k-NN accuracy for different k values\"\"\"\n",
    "    results={}\n",
    "    all_errors={}\n",
    "\n",
    "    for k in k_values:\n",
    "        correct=0\n",
    "        total=len(test_embeddings)\n",
    "        errors=[]\n",
    "\n",
    "        for i in tqdm(range(total),desc=f\"Evaluatingk={k}\"):\n",
    "            query_embedding=test_embeddings[i:i+1].astype(\"float32\")\n",
    "            true_label=test_metadata[i][\"label\"]\n",
    "            true_message=test_metadata[i][\"message\"]\n",
    "\n",
    "            #SearchinFAISSindex\n",
    "            scores,indices=index.search(query_embedding,k)  \n",
    "\n",
    "            #Getpredictionsfrom top-kneighbors\n",
    "            predictions=[]\n",
    "            neighbor_details=[]\n",
    "            for j in range(k):\n",
    "                neighbor_idx=indices[0][j]\n",
    "                neighbor_label= train_metadata[neighbor_idx][\"label\"]\n",
    "                neighbor_message = train_metadata[neighbor_idx][\"message\"]\n",
    "                neighbor_score=float(scores[0][j])  \n",
    "                predictions.append(neighbor_label)\n",
    "                neighbor_details.append({\n",
    "                    \"label\":neighbor_label,\n",
    "                    \"message\":neighbor_message,\n",
    "                    \"score\":neighbor_score\n",
    "                })\n",
    "            \n",
    "            # Majority vote\n",
    "            unique_labels, counts = np.unique(predictions, return_counts=True)\n",
    "            predicted_label = unique_labels[np.argmax(counts)]\n",
    "            if predicted_label == true_label:\n",
    "                correct += 1\n",
    "            else:\n",
    "                # Collect error information\n",
    "                error_info = {\n",
    "                    \"index\": i,\n",
    "                    \"original_index\": test_metadata[i][\"index\"],\n",
    "                    \"message\": true_message,\n",
    "                    \"true_label\": true_label,\n",
    "                    \"predicted_label\": predicted_label,\n",
    "                    \"neighbors\": neighbor_details,\n",
    "                }\n",
    "                errors.append(error_info)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        error_count = total- correct\n",
    "        \n",
    "        results[k] = accuracy\n",
    "        all_errors[k] = errors\n",
    "\n",
    "        print(f\"Accuracy with k={k}: {accuracy:.4f}\")\n",
    "        print(f\"Number of errors with k={k}: {error_count}/{total} ({(error_count/total\n",
    "        )*100:.2f}%)\")\n",
    "\n",
    "    return results, all_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b1890ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating accuracy on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluatingk=1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 1646.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=1: 0.9857\n",
      "Number of errors with k=1: 8/558 (1.43%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluatingk=2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 2032.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=2: 0.9875\n",
      "Number of errors with k=2: 7/558 (1.25%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluatingk=3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 1984.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=3: 0.9928\n",
      "Number of errors with k=3: 4/558 (0.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluatingk=4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 1903.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=4: 0.9892\n",
      "Number of errors with k=4: 6/558 (1.08%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluatingk=5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 1942.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=5: 0.9910\n",
      "Number of errors with k=5: 5/558 (0.90%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluatingk=6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 1967.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=6: 0.9892\n",
      "Number of errors with k=6: 6/558 (1.08%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluatingk=7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 1839.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=7: 0.9892\n",
      "Number of errors with k=7: 6/558 (1.08%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluatingk=8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 1959.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=8: 0.9875\n",
      "Number of errors with k=8: 7/558 (1.25%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluatingk=9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 1974.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=9: 0.9875\n",
      "Number of errors with k=9: 7/558 (1.25%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluatingk=10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 1935.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=10: 0.9892\n",
      "Number of errors with k=10: 6/558 (1.08%)\n",
      "\n",
      "==================================================\n",
      "ACCURACY RESULTS\n",
      "==================================================\n",
      "Top-1 accuracy: 0.9857 (98.57%)\n",
      "Top-2 accuracy: 0.9875 (98.75%)\n",
      "Top-3 accuracy: 0.9928 (99.28%)\n",
      "Top-4 accuracy: 0.9892 (98.92%)\n",
      "Top-5 accuracy: 0.9910 (99.10%)\n",
      "Top-6 accuracy: 0.9892 (98.92%)\n",
      "Top-7 accuracy: 0.9892 (98.92%)\n",
      "Top-8 accuracy: 0.9875 (98.75%)\n",
      "Top-9 accuracy: 0.9875 (98.75%)\n",
      "Top-10 accuracy: 0.9892 (98.92%)\n",
      "==================================================\n",
      "\n",
      "*** Error analysis saved to: error_analysis.json ***\n",
      "\n",
      "*** Summary ***\n",
      "k=1: 8 errors out of 558 samples\n",
      "k=2: 7 errors out of 558 samples\n",
      "k=3: 4 errors out of 558 samples\n",
      "k=4: 6 errors out of 558 samples\n",
      "k=5: 5 errors out of 558 samples\n",
      "k=6: 6 errors out of 558 samples\n",
      "k=7: 6 errors out of 558 samples\n",
      "k=8: 7 errors out of 558 samples\n",
      "k=9: 7 errors out of 558 samples\n",
      "k=10: 6 errors out of 558 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. ƒê√°nh gi√° accuracy tr√™n test set\n",
    "\n",
    "print(\"Evaluating accuracy on test set...\")\n",
    "accuracy_results, error_results = evaluate_knn_accuracy(X_test_emb, test_metadata, index, train_metadata, k_values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ACCURACY RESULTS\")\n",
    "print(\"=\"*50)\n",
    "for k, accuracy in accuracy_results.items():\n",
    "    print(f\"Top-{k} accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# L∆∞u ph√¢n t√≠ch l·ªói ra file\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Prepare error analysis dictionary\n",
    "error_analysis = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"test_size\": len(X_test_emb),\n",
    "    \"accuracy_results\": accuracy_results,\n",
    "    \"errors_by_k\": {}\n",
    "}\n",
    "\n",
    "# Populate errors by value of k\n",
    "for k, errors in error_results.items():\n",
    "    error_analysis[\"errors_by_k\"][f\"k_{k}\"] = {\n",
    "        \"total_errors\": len(errors),\n",
    "        \"errors\": errors,\n",
    "        \"error_rate\": len(errors) / len(X_test_emb)\n",
    "    }\n",
    "\n",
    "# Save error analysis to JSON file\n",
    "output_file = \"error_analysis.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(error_analysis, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n*** Error analysis saved to: {output_file} ***\\n\")\n",
    "print(\"*** Summary ***\")\n",
    "for k, errors in error_results.items():\n",
    "    print(f\"k={k}: {len(errors)} errors out of {len(X_test_emb)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e796e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model components...\n",
      "‚úì Saved models\\faiss_index.bin\n",
      "‚úì Saved models\\train_metadata.pkl\n",
      "‚úì Saved models\\dl_label_encoder.pkl\n",
      "‚úì Saved models\\dl_model_config.pkl\n",
      "\n",
      "üéØ Summary of saved files in 'models' folder:\n",
      "============================================================\n",
      "DL Model Components:\n",
      "- models/faiss_index.bin (FAISS similarity search index)\n",
      "- models/train_metadata.pkl (Training data metadata)\n",
      "- models/dl_label_encoder.pkl (Label encoder for DL approach)\n",
      "- models/dl_model_config.pkl (DL model configuration and performance)\n",
      "\n",
      "Note: The embedding model will be loaded from HuggingFace using model name: intfloat/multilingual-e5-base\n",
      "üèÜ Best k value: k=3 with accuracy: 0.9928\n"
     ]
    }
   ],
   "source": [
    "# Save model components for reuse\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = \"models\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"‚úì Created directory: {models_dir}\")\n",
    "\n",
    "print(\"Saving model components...\")\n",
    "\n",
    "# Save the FAISS index\n",
    "faiss_index_path = os.path.join(models_dir, \"faiss_index.bin\")\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "print(f\"‚úì Saved {faiss_index_path}\")\n",
    "\n",
    "# Save train metadata\n",
    "train_metadata_path = os.path.join(models_dir, \"train_metadata.pkl\")\n",
    "with open(train_metadata_path, 'wb') as f:\n",
    "    pickle.dump(train_metadata, f)\n",
    "print(f\"‚úì Saved {train_metadata_path}\")\n",
    "\n",
    "# Save label encoder with DL-specific name to avoid conflicts\n",
    "dl_label_encoder_path = os.path.join(models_dir, \"dl_label_encoder.pkl\")\n",
    "with open(dl_label_encoder_path, 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "print(f\"‚úì Saved {dl_label_encoder_path}\")\n",
    "\n",
    "# Save model configuration info\n",
    "model_config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"test_size\": TEST_SIZE,\n",
    "    \"seed\": SEED,\n",
    "    \"accuracy_results\": accuracy_results\n",
    "}\n",
    "\n",
    "dl_model_config_path = os.path.join(models_dir, \"dl_model_config.pkl\")\n",
    "with open(dl_model_config_path, 'wb') as f:\n",
    "    pickle.dump(model_config, f)\n",
    "print(f\"‚úì Saved {dl_model_config_path}\")\n",
    "\n",
    "print(f\"\\nüéØ Summary of saved files in '{models_dir}' folder:\")\n",
    "print(\"=\"*60)\n",
    "print(\"DL Model Components:\")\n",
    "print(f\"- {models_dir}/faiss_index.bin (FAISS similarity search index)\")\n",
    "print(f\"- {models_dir}/train_metadata.pkl (Training data metadata)\")\n",
    "print(f\"- {models_dir}/dl_label_encoder.pkl (Label encoder for DL approach)\")\n",
    "print(f\"- {models_dir}/dl_model_config.pkl (DL model configuration and performance)\")\n",
    "\n",
    "print(f\"\\nNote: The embedding model will be loaded from HuggingFace using model name: {MODEL_NAME}\")\n",
    "print(f\"üèÜ Best k value: k={max(accuracy_results, key=accuracy_results.get)} with accuracy: {max(accuracy_results.values()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0569b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model components from models folder...\n",
      "Loaded model: intfloat/multilingual-e5-base\n",
      "Training samples: 5014\n",
      "Device: cpu\n",
      "Best performance: k=3 with accuracy=0.9928\n",
      "Ready for inference!\n"
     ]
    }
   ],
   "source": [
    "### Can run from here if you have the saved files\n",
    "\n",
    "# Load saved model components (for inference without retraining)\n",
    "# This cell can be used by others who receive your saved files\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import faiss\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def load_model_components():\n",
    "    \"\"\"Load all saved model components for inference\"\"\"\n",
    "    print(\"Loading saved model components from models folder...\")\n",
    "    \n",
    "    models_dir = \"models\"\n",
    "    \n",
    "    # Load configuration\n",
    "    dl_model_config_path = os.path.join(models_dir, 'dl_model_config.pkl')\n",
    "    with open(dl_model_config_path, 'rb') as f:\n",
    "        config = pickle.load(f)\n",
    "    \n",
    "    # Load the embedding model and tokenizer from HuggingFace\n",
    "    MODEL_NAME = config[\"model_name\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Set device and prepare model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load FAISS index\n",
    "    faiss_index_path = os.path.join(models_dir, \"faiss_index.bin\")\n",
    "    index = faiss.read_index(faiss_index_path)\n",
    "    \n",
    "    # Load metadata\n",
    "    train_metadata_path = os.path.join(models_dir, 'train_metadata.pkl')\n",
    "    with open(train_metadata_path, 'rb') as f:\n",
    "        train_metadata = pickle.load(f)\n",
    "    \n",
    "    # Load label encoder (DL-specific filename)\n",
    "    dl_label_encoder_path = os.path.join(models_dir, 'dl_label_encoder.pkl')\n",
    "    with open(dl_label_encoder_path, 'rb') as f:\n",
    "        le = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded model: {MODEL_NAME}\")\n",
    "    print(f\"Training samples: {len(train_metadata)}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    if 'accuracy_results' in config:\n",
    "        best_k = max(config['accuracy_results'], key=config['accuracy_results'].get)\n",
    "        best_accuracy = config['accuracy_results'][best_k]\n",
    "        print(f\"Best performance: k={best_k} with accuracy={best_accuracy:.4f}\")\n",
    "    print(\"Ready for inference!\")\n",
    "    \n",
    "    return model, tokenizer, device, index, train_metadata, le\n",
    "\n",
    "# Uncomment the line below to load components (useful when starting fresh)\n",
    "model, tokenizer, device, index, train_metadata, le = load_model_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b142c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_knn(query_text, model, tokenizer, device, index, train_metadata, k=1):\n",
    "    \"\"\"Classify text using k-nearest neighbors with embeddings\"\"\"\n",
    "    # Get query embedding\n",
    "    query_with_prefix = f\"query: {query_text}\"\n",
    "    batch_dict = tokenizer([query_with_prefix],\n",
    "    max_length=512,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\")\n",
    "    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch_dict)\n",
    "        query_embedding = average_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n",
    "        query_embedding = F.normalize(query_embedding, p=2, dim=1)\n",
    "        query_embedding = query_embedding.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    scores, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Let's manually check if we can compute similarity with first training sample\n",
    "    if len(train_metadata) > 0:\n",
    "        # Get the first training embedding from FAISS\n",
    "        first_train_embedding = index.reconstruct(0).reshape(1, -1)\n",
    "        manual_similarity = np.dot(query_embedding, first_train_embedding.T)[0, 0]\n",
    "    \n",
    "    # Get predictions from top-k neighbors\n",
    "    predictions = []\n",
    "    neighbor_info = []\n",
    "\n",
    "    for i in range(k):\n",
    "        neighbor_idx=indices[0][i]\n",
    "        neighbor_score=float(scores[0][i])  # Convert to Python float explicitly\n",
    "        neighbor_label=train_metadata[neighbor_idx][\"label\"]\n",
    "        neighbor_message=train_metadata[neighbor_idx][\"message\"]\n",
    "\n",
    "        predictions.append(neighbor_label)\n",
    "        neighbor_info.append({\n",
    "           \"score\":neighbor_score,\n",
    "           \"label\":neighbor_label,\n",
    "           \"message\":neighbor_message[:100]+\"...\"if len(neighbor_message)>100 else neighbor_message })\n",
    "\n",
    "    #Majority vote for final prediction\n",
    "    unique_labels,counts=np.unique(predictions,return_counts=True)\n",
    "    final_prediction=unique_labels[np.argmax(counts)]\n",
    "    score = np.mean([info[\"score\"] for info in neighbor_info])  # Average score of neighbors\n",
    "    return final_prediction, neighbor_info, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b635ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Pipeline classification for user input\n",
    "def spam_classifier_pipeline(user_input, k=3):\n",
    "    \"\"\"\n",
    "    Complete pipeline for spam classification.\n",
    "\n",
    "    Args:\n",
    "        user_input (str): Text to classify\n",
    "        k (int): Number of nearest neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "        dict: Classification results with details\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(f'*** Classifying: \"{user_input}\"')\n",
    "    print(f\"*** Using top-{k} nearest neighbors\")\n",
    "    print()\n",
    "\n",
    "    # Get prediction and neighbors\n",
    "    prediction, neighbors, score = classify_with_knn(\n",
    "        user_input, model, tokenizer, device, index, train_metadata, k=k\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(f\"*** Prediction: {prediction.upper()} | Score: {score:.6f}\")\n",
    "    print()\n",
    "    print(\"*** Top neighbors:\")\n",
    "    for i, neighbor in enumerate(neighbors, 1):\n",
    "        # Use more decimal places for score display\n",
    "        print(f\"\\n{i}. Label: {neighbor['label']} | Score: {neighbor['score']:.6f}\")\n",
    "        print(f\"   Message: {neighbor['message']}\")\n",
    "\n",
    "    # Count label distribution\n",
    "    labels = [n[\"label\"] for n in neighbors]\n",
    "    label_counts = {label: labels.count(label) for label in set(labels)}\n",
    "\n",
    "    return {\n",
    "        \"prediction\": prediction,\n",
    "        \"score\": score,\n",
    "        \"neighbors\": neighbors,\n",
    "        \"label_distribution\": label_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ff2a3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1: \"I am actually thinking a way of doing something useful\" ---\n",
      "\n",
      "*** Classifying: \"I am actually thinking a way of doing something useful\"\n",
      "*** Using top-3 nearest neighbors\n",
      "\n",
      "*** Prediction: HAM | Score: 0.839318\n",
      "\n",
      "*** Top neighbors:\n",
      "\n",
      "1. Label: ham | Score: 0.842366\n",
      "   Message: yeah, that's what I was thinking\n",
      "\n",
      "2. Label: ham | Score: 0.841213\n",
      "   Message: that would be good ‚Ä¶ I'll phone you tomo lunchtime, shall I, to organise something?\n",
      "\n",
      "3. Label: ham | Score: 0.834373\n",
      "   Message: See? I thought it all through\n",
      "\n",
      "--- Example 2: \"FREE!! Click here to win \\$1000 NOW! Limited time offer!\" ---\n",
      "\n",
      "*** Classifying: \"FREE!! Click here to win \\$1000 NOW! Limited time offer!\"\n",
      "*** Using top-3 nearest neighbors\n",
      "\n",
      "*** Prediction: SPAM | Score: 0.851784\n",
      "\n",
      "*** Top neighbors:\n",
      "\n",
      "1. Label: spam | Score: 0.856560\n",
      "   Message: Win a ¬£1000 cash prize or a prize worth ¬£5000\n",
      "\n",
      "2. Label: spam | Score: 0.849934\n",
      "   Message: FREE entry into our ¬£250 weekly competition just text the word WIN to 80086 NOW. 18 T&C www.txttowin...\n",
      "\n",
      "3. Label: spam | Score: 0.848858\n",
      "   Message: FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For...\n",
      "\n",
      "--- Interactive Testing ---\n",
      "\n",
      "*** Classifying: \"Win a free iPhone! Click here now!\"\n",
      "*** Using top-5 nearest neighbors\n",
      "\n",
      "*** Prediction: SPAM | Score: 0.857195\n",
      "\n",
      "*** Top neighbors:\n",
      "\n",
      "1. Label: spam | Score: 0.863312\n",
      "   Message: FREE entry into our ¬£250 weekly competition just text the word WIN to 80086 NOW. 18 T&C www.txttowin...\n",
      "\n",
      "2. Label: spam | Score: 0.860447\n",
      "   Message: FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For...\n",
      "\n",
      "3. Label: spam | Score: 0.860447\n",
      "   Message: FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For...\n",
      "\n",
      "4. Label: spam | Score: 0.851080\n",
      "   Message: U have won a nokia 6230 plus a free digital camera. This is what u get when u win our FREE auction. ...\n",
      "\n",
      "5. Label: spam | Score: 0.850688\n",
      "   Message: TheMob>Yo yo yo-Here comes a new selection of hot downloads for our members to get for FREE! Just cl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\$'\n",
      "C:\\Users\\votaq\\AppData\\Local\\Temp\\ipykernel_37368\\2609611235.py:4: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  \"FREE!! Click here to win \\$1000 NOW! Limited time offer!\"\n",
      "c:\\Coding\\AIOprj2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 7. Test pipeline with various examples\n",
    "test_examples = [\n",
    "    \"I am actually thinking a way of doing something useful\",\n",
    "    \"FREE!! Click here to win \\$1000 NOW! Limited time offer!\"\n",
    "]\n",
    "\n",
    "# Run the classifier on each test example\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    print(f\"\\n--- Example {i}: \\\"{example}\\\" ---\")\n",
    "    result = spam_classifier_pipeline(example, k=3) # k=3 is the best performing value from previous tests\n",
    "\n",
    "# Interactive testing ‚Äì user can change text and k value\n",
    "print(\"\\n--- Interactive Testing ---\")\n",
    "user_text = \"Win a free iPhone! Click here now!\"\n",
    "k_value = 5\n",
    "result = spam_classifier_pipeline(user_text, k=k_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
